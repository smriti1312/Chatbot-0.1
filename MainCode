# -*- coding: utf-8 -*-
"""Chatbot 0.1 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1**********7R
"""

pip install llama-index

pip install langchain

pip install gradio

from llama_index import SimpleDirectoryReader,GPTListIndex,GPTVectorStoreIndex,LLMPredictor,PromptHelper,ServiceContext,StorageContext,load_index_from_storage
from langchain import OpenAI
from langchain.chat_models import ChatOpenAI
import sys
import os
import openai
import gradio as gr

openai.organization = "org-**********" #organisation id
os.environ["OPENAI_API_KEY"] = 'sk-********' #APIKey
openai.api_key = os.environ["OPENAI_API_KEY"]

!wget https://drive.google.com/file/d/************/view?usp=sharing #retrieves file

print("yeah")

def index(path):
    max_input = 4096
    tokens = 200
    chunk_size = 600  # for LLM, we need to define chunk size
    max_chunk_overlap = 1
    num_outputs= 512


    # Define prompt
    promptHelper = PromptHelper(max_input, tokens, max_chunk_overlap, chunk_size_limit=chunk_size)

    # Define LLM — there could be many models we can use, but in this example, let’s go with OpenAI model
    llmPredictor = LLMPredictor(llm=OpenAI(temperature=0.7, model_name="text-ada-001", max_tokens=num_outputs))
    print("File is going to be read")

    docs = SimpleDirectoryReader(path).load_data()

    print("File Read Successfully")

    # Create vector index
    service_context = ServiceContext.from_defaults(llm_predictor=llmPredictor, prompt_helper=promptHelper)

    vectorIndex = GPTVectorStoreIndex.from_documents(documents=docs, service_context=service_context)
    vectorIndex.storage_context.persist(persist_dir='Data')
    print("Indexing done successfully")

index("Data")

def answer(question):

    storage_context = StorageContext.from_defaults(persist_dir='Data')
    index = load_index_from_storage(storage_context)

    print("query done")
    query_engine = index.as_query_engine()
    response = query_engine.query(question)
    print("respone fed")
    return response

response = answer("Dell")

print(response)

iface = gr.Interface(fn= answerMe,
                     inputs=gr.inputs.Textbox(lines=7, label="Enter your text"),
                     outputs="text",
                     title="My AI Chatbot")

index = create_index("docs")
iface.launch(share=True)
